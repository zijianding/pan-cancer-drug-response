####for CNV data###
###system parameters###
#options(digits = 15)

####functions###
bootstrap_sample <- function(obs,class,BS){
  #this function only generates BS samples
  #in each sample, two classes are balanced
  #the sample size equals to the number of obs(+1/-1 due to odd-even)
  #return a list
  resp = unique(class)
  
  pos_obs = obs[class==resp[2]]
  neg_obs = obs[class==resp[1]]
  sample.num = round((length(pos_obs)+length(neg_obs))/2)
  
  bs_mat.sample = matrix("NULL",ncol=BS,nrow=(2*sample.num))
  bs_mat.resp = matrix("NULL",ncol=BS,nrow=(2*sample.num))
  
  for(bs in 1:BS)
  {
    #sample positive observations
    sample_pos = sample(pos_obs,size=sample.num,replace=T)
    sample_pos_resp = class[match(sample_pos,obs)]
    #sample_pos_resp = as.character( train.info$response[match(sample_pos,as.character(train.info$patient))] )
    #sample negative observations
    sample_neg = sample(neg_obs,size=sample.num,replace=T)
    sample_neg_resp = class[match(sample_neg,obs)]
    #sample_neg_resp = as.character( train.info$response[match(sample_neg,as.character(train.info$patient))] )
    bs_mat.sample[,bs] = c(sample_pos,sample_neg)
    bs_mat.resp[,bs] = c(sample_pos_resp,sample_neg_resp)
  }
  
  bs_mat = list(bs_mat.sample,bs_mat.resp)
  return(bs_mat)
}

partition_data <- function(obs, k){
  #k-fold partition of obs
  cv.mat = matrix("NULL",nrow=length(obs),ncol=k)
  
  fd.size = floor(length(obs)/k)
  tmp = length(obs)%%k
  if(tmp>0)
  {
    fd.ix = rep(1:tmp,each = (fd.size+1))
    fd.ix = c(fd.ix,rep((tmp+1):k,each=fd.size))
  }
  if(tmp==0)
  {
    fd.ix = rep(1:k,each=fd.size)
  }
  
  for( j in 1:k)
  {
    cv.mat[fd.ix!=j,j] = "train"
    cv.mat[fd.ix==j,j] = "validation"
  }
  rownames(cv.mat) = obs
  return(cv.mat)
}

comb <- function(x, ...) {
  lapply(seq_along(x),
         function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
}

#for debug
# train_dat=train.dat; test_dat = test.dat;info=cisplatin.info 
# type="regression"; parallel=T
#
test_gene <- function(train.dat, test.dat, sig_gene=50, p_thresh=0.05,type="ttest")
{
  #data are generated by
  if(type=="ttest")
  {
    pvalue = vector(length=ncol(train.dat)-1,mode="numeric")
    ix.1 = which(train.dat$Class=="Class1")
    ix.2 = which(train.dat$Class=="Class2")
    for( i in 1:(ncol(train.dat)-1) )
    {
      dat.1 = train.dat[ix.1,i]
      dat.2 = train.dat[ix.2,i]
      test_fit = t.test(dat.1,dat.2,na.action="na.omit")
      pvalue[i] = test_fit$p.value
    }
    ix = which(pvalue<=0.05)
    ix = c(ix,ncol(train.dat))
    return( list(train.dat[,ix],test.dat[,ix],length(ix)-1) )
  }
}

gene_selection <- function(data,freq=0.8)
{
  #data: row as BS and column as genes
  #new_data = data
  #new_data[new_data!=0] = 1
  
  #direction
  data[data>0] = 1
  data[data<0] = -1
  #times of larger/smaller than 0
  up = colSums(data>0)
  down = colSums(data<0)
  #the largest times and freq
  side_max = abs(up-down)
  side_freq = side_max/nrow(data)
  names(side_freq) = colnames(data)
  
  
  #feature_freq = colSums(new_data)
  #feature_freq = feature_freq/nrow(data)
  
  
  ix = which( side_freq>=freq )
  
  genes = colnames(data)[ix]
  
  return(list(genes,ix,side_freq))
  
}

ensemble_roc <- function(score_mat,class,target_class,step=0.0001)
{
  #score_mat: row as one model, column as patients
  #class: responses
  #return value: tpr and fpr under each cut
  #cut: seq(0,1,by=0.01)
  #currently target class is treated as negative classs
  #which is wrong, later we will corrected this
  # for debug
  #     score_mat = t(test_score)
  #     class = test.resp
  #     target_class = "sensitive"
  #     step=0.0001
  
  #
  
  
  #here is the problem
  class_1 = target_class # large score
  class_2 = setdiff( unique(class), target_class )
  #   if( sum(class==class_1) > sum(class==class_2) )
  #   {
  #     class_p = class_2
  #     class_n = class_1
  #   }
  #   if( sum(class==class_1) <= sum(class==class_2) )
  #   {
  #     class_p = class_1
  #     class_n = class_2
  #   }
  class_p = class_2 #small score
  class_n = class_1 #large score
  
  
  
  cut_vec = seq(0,1+step,by=step)
  cut_vec[length(cut_vec)] = Inf
  tpr_vec = vector(length=length(cut_vec),mode="numeric")
  fpr_vec = vector(length=length(cut_vec),mode="numeric")
  
  for( i in 1:length(cut_vec) )
  {
    curr_mat = matrix( NA,nrow=nrow(score_mat),ncol=ncol(score_mat) )
    curr_mat[ score_mat>=cut_vec[i] ] = 1 #large score
    curr_mat[ score_mat< cut_vec[i] ] = 0 #small score
    
    curr_score = colMeans( curr_mat )
    
    curr_class = vector(length=length(curr_score),mode="character")
    curr_class[curr_score>0.5] = class_1
    curr_class[curr_score<=0.5] = class_2
    
    #mytable = table( curr_class,class )
    mytable = matrix(0,nrow=2,ncol=2)
    dimnames(mytable) = list( c(class_p,class_n), c(class_p,class_n) )
    
    mytable[1,1] = sum(curr_class[class==class_p]==class_p)
    mytable[2,1] = sum(curr_class[class==class_p]==class_n)
    mytable[1,2] = sum(curr_class[class==class_n]==class_p)
    mytable[2,2] = sum(curr_class[class==class_n]==class_n)
    
    
    if( sum(mytable[,1])!=0 )
    {
      tpr_vec[i] = mytable[1,1]/sum(mytable[,1])
    }
    if( sum(mytable[,1])==0 )
    {
      tpr_vec[i] = NA 
    }
    if( sum(mytable[,2])!=0 )
    {
      fpr_vec[i] = mytable[1,2]/sum(mytable[,2])
    }
    if( sum(mytable[,2])==0 )
    {
      fpr_vec[i] = NA
    }
    
  }
  
  roc_mat = cbind(tpr_vec,fpr_vec)
  colnames(roc_mat) = c("tpr","fpr")
  return(roc_mat)
  
}


####load data####
#libraries
library(glmnet)
library(doParallel)
library(foreach)
library(pracma)
no_cores = detectCores()


##find data##
#artifical test data#
library(caret)
set.seed(1)
#200 positive, 10 features
#dat <- twoClassSim(1000, intercept = -10,linearVars=3,noiseVars=1,corrVars=1)
#501 positive, 10 features
#dat <- twoClassSim(1000, intercept = -4,linearVars=3,noiseVars=1,corrVars=1)
#480 positive, 1000 features
#dat <- twoClassSim(1000, intercept = -5,linearVars=200,noiseVars=700,corrVars=95)
#201 positive, 1000 features
#dat <- twoClassSim(1000, intercept = -22,linearVars=200,noiseVars=700,corrVars=95)
sum(dat$Class=="Class1")
sum(dat$Class=="Class2")
#dat <- twoClassSim(1000, intercept = -10,linearVars=1000,noiseVars=50,corrVars=10)
datpart = createDataPartition(y=dat$Class, times=1, p=0.5, list=F)
train.dat = dat[datpart[,1],]
test.dat = dat[-datpart[,1],]
#test.dat <- twoClassSim(5000, intercept = -50,linearVars=1000,noiseVars=50,corrVars=10)

# when no caret
# 500 train(400 Class1,100 class2), 500 test(400 class1, 100 class2); 
# 10 variates related to class; 90 non-related to class



##select differential features##
list_tmp = test_gene(train.dat, test.dat)
train.dat = list_tmp[[1]]
test.dat = list_tmp[[2]]
feature_num = list_tmp[[3]]
print(feature_num)


###estimated robust features###
#basic parameters#
BS = 100
alphas = seq(0.1,1,by=0.1)

#bootstrap samples#
#train patients response
#train.resp = as.character(train.info$response[match(colnames(train.dat),as.character(train.info$patient))])
test.resp = as.character(test.dat$Class)
train.resp = as.character(train.dat$Class)
list.bs_mat = bootstrap_sample( rownames(train.dat) , train.resp, BS=BS)
bs_mat.pats = list.bs_mat[[1]]
bs_mat.resp = list.bs_mat[[2]]

##estimates recurrent features##
#model training#
best_lambda = matrix(NA,nrow=length(alphas),ncol=BS)
best_auc = matrix(NA,nrow=length(alphas),ncol=BS)
for(i in 1:length(alphas))
{
  cl = makeCluster(no_cores)
  registerDoParallel(cl)
  train_list <- foreach( bs=1:BS, .packages="glmnet",
                         .combine='comb', .multicombine=TRUE,
                         .init=list(list(), list()) ) %dopar%
  {
    #bootstrap sample data
    curr.train_dat = train.dat[ match(bs_mat.pats[,bs],rownames(train.dat) ) , 
                                1:(ncol(train.dat)-1)]
    curr.train_resp = bs_mat.resp[,bs]
  
    #record the best models in each bootstrap sample
    cv_fit = cv.glmnet( as.matrix(curr.train_dat), as.factor(curr.train_resp), 
                        family="binomial", type.measure="auc" )
  
    ix = match(cv_fit$lambda.1se,cv_fit$lambda)
    #beta = cv_fit$glmnet.fit$beta[,ix]
    auc = cv_fit$cvm[ix]
    return( list(cv_fit$lambda.1se,auc) )
  }
  stopImplicitCluster()
  stopCluster(cl)
  
  curr_lambda = unlist(train_list[[1]])
  curr_auc = unlist( train_list[[2]] )
  
  best_lambda[i,] = curr_lambda
  best_auc[i,] = curr_auc
}

#find best models and corresponding features
cl = makeCluster(no_cores)
registerDoParallel(cl)
best_beta <- foreach(bs=1:BS,.combine='cbind',
                     .packages="glmnet") %dopar%
{
  #best model
  ix = which.max(best_auc[,bs])
  alpha = alphas[ix]
  lambda = best_lambda[ix,bs]
  
  #bootstrap data
  curr.train_dat = train.dat[ match(bs_mat.pats[,bs],rownames(train.dat) ) , 
                              1:(ncol(train.dat)-1)]
  curr.train_resp = bs_mat.resp[,bs]
  
  #train model
  glm_fit = glmnet( as.matrix(curr.train_dat), as.factor(curr.train_resp), 
                    family="binomial",alpha=alpha,lambda=lambda)
  
  curr_beta = as.vector(glm_fit$beta)
  
  return(curr_beta)
}
stopImplicitCluster()
stopCluster(cl)


#find recurrent features#
rownames(best_beta) = colnames(train.dat)[-ncol(train.dat)]
freq = 0.9
list_features = gene_selection(t(best_beta),freq=freq)
while( length(list_features[[2]]) <= 1 )
{
  freq = freq - 0.05
  list_features = gene_selection(t(best_beta),freq=freq)
}
tmp_str = paste( "Frequency ",freq," to select recurrent features:",length(list_features[[2]]) )
print(tmp_str)

#refine data by recurrent features
traindat.rev = train.dat
testdat.rev = test.dat
train.dat = train.dat[,list_features[[2]]]
test.dat = test.dat[,list_features[[2]]]
feature_freq = list_features[[3]]


##refit models and test##
#bootstrap samples#
list.bs_mat = bootstrap_sample( rownames(train.dat) , train.resp, BS=BS)
bs_mat.pats = list.bs_mat[[1]]
bs_mat.resp = list.bs_mat[[2]]

#fit models and test
cl = makeCluster(no_cores)
registerDoParallel(cl)
test_list <- foreach(bs=1:BS,.packages="glmnet") %dopar%
{
  #bootstrap sample
  curr.train_dat = train.dat[ match(bs_mat.pats[,bs],rownames(train.dat)) , ]
  curr.train_resp = as.factor(bs_mat.resp[,bs])
  
  glm_fit = glmnet(as.matrix(curr.train_dat),y=as.factor(curr.train_resp),
               lambda=0, family="binomial")
  
  #predict on test data
  pred = predict( object=glm_fit, type="response",
                  newx=as.matrix(test.dat) )

  return(as.vector(pred))
  
}
stopImplicitCluster()
stopCluster(cl)

test_score = do.call(cbind,test_list)
test.pats = rownames(test.dat)
rownames(test_score) = test.pats


###output results###
#feature frequncy#
tmp_str = paste(output_folder,"/pan.elanet.feature_freq.test_",test_fold-3,".20150701.tiff",sep="")
tiff(tmp_str)
hist(feature_freq,main="Frequency of hittring for genes",xlab="hitting Freq",ylab="Freq",50)
dev.off()

##final results##
##plot TEST performance##
# tmp_str = paste(output_folder,"/pan.elanat.test_",test_fold-3,".20150708.tiff",sep="")
# tiff(tmp_str)
#roc
roc = ensemble_roc(t(test_score),test.resp,"Class2")
plot(roc[,2],roc[,1],"l",cex=0.8,xlab="FPR",ylab="TPR",
     xlim=c(0,1),ylim=c(0,1))
auc = trapz(roc[,2],roc[,1])
tmp_str = paste("AUC = ",round(auc,digits=2),sep="")
tmp_str = paste(tmp_str,";NO.Feature=",length(list_features[[1]]),sep="")
title("Linear Model on Artificial data",tmp_str)
lines(seq(0,1,0.01),seq(0,1,0.01),lty=2,col="gray")
# dev.off()
#200 positive, 10 features
# "TwoFactor2""Linear2"
# write.table(roc,"roc.200_pos.10_features.2_selected.txt",
#             sep="\t",row.names=F,col.names=F)
#501 positive, 10 features
# "TwoFactor1" "TwoFactor2" "Linear2"    "Linear3"
write.table(roc,"roc.501_pos.10_features.4_selected.txt",
            sep="\t",row.names=F,col.names=F)
#480 positive,1000 features
# [1] "Linear002" "Linear003" "Linear004" "Linear005" "Linear006" "Linear008" "Linear009" "Linear011"
# [9] "Linear013" "Linear016" "Linear021" "Linear025" "Linear027" "Linear031" "Linear033" "Linear035"
# [17] "Linear039" "Linear041" "Linear042" "Linear043" "Linear053" "Linear055" "Linear058" "Linear063"
# [25] "Linear067" "Linear068" "Linear076" "Linear078" "Linear089" "Linear091" "Linear092" "Linear097"
# [33] "Linear103" "Linear104" "Linear107" "Linear108" "Linear140" "Noise068"  "Noise070"  "Noise094" 
# [41] "Noise153"  "Noise176"  "Noise221"  "Noise245"  "Noise367"  "Noise375"  "Noise461"  "Noise482" 
# [49] "Noise618"  "Noise662"  "Noise689"  "Corr35" 
write.table(roc,"roc.480_pos.1000_feature.52_selected.txt",
            sep="\t",row.names=F,col.names=F)

#209 positive,1000 features
# [1] "Linear006" "Linear011" "Linear018" "Linear022" "Linear027" "Linear031" "Linear032" "Linear033"
# [9] "Linear035" "Linear042" "Linear050" "Linear056" "Linear079" "Linear100" "Linear110" "Linear122"
# [17] "Linear146" "Linear157" "Linear158" "Linear171" "Noise019"  "Noise022"  "Noise070"  "Noise152" 
# [25] "Noise171"  "Noise198"  "Noise238"  "Noise300"  "Noise303"  "Noise326"  "Noise348"  "Noise394" 
# [33] "Noise414"  "Noise417"  "Noise437"  "Noise438"  "Noise480"  "Noise609"  "Noise610"  "Corr41"   
# [41] "Corr42" 
write.table(roc,"roc.209_pos.1000_feature.41_selected.txt",
            sep="\t",row.names=F,col.names=F)

####THE END####


